<div align="center">
<h1 style="border-bottom: none; margin-bottom: 0px ">Depth Anything 3: Recovering the Visual Space from Any Views</h1>
<!-- <h2 style="border-top: none; margin-top: 3px;">Recovering the Visual Space from Any Views</h2> -->


[**Haotong Lin**](https://haotongl.github.io/)<sup>&ast;</sup> Â· [**Sili Chen**](https://github.com/SiliChen321)<sup>&ast;</sup> Â· [**Jun Hao Liew**](https://liewjunhao.github.io/)<sup>&ast;</sup> Â· [**Donny Y. Chen**](https://donydchen.github.io)<sup>&ast;</sup> Â· [**Zhenyu Li**](https://zhyever.github.io/) Â· [**Guang Shi**](https://scholar.google.com/citations?user=MjXxWbUAAAAJ&hl=en) Â· [**Jiashi Feng**](https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en)
<br>
[**Bingyi Kang**](https://bingykang.github.io/)<sup>&ast;&dagger;</sup>

&dagger;project lead&emsp;&ast;Equal Contribution

<a href="https://arxiv.org/abs/2511.10647"><img src='https://img.shields.io/badge/arXiv-Depth Anything 3-red' alt='Paper PDF'></a>
<a href='https://depth-anything-3.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything 3-green' alt='Project Page'></a>
<a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-3'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>
<!-- <a href='https://huggingface.co/datasets/depth-anything/VGB'><img src='https://img.shields.io/badge/Benchmark-VisGeo-yellow' alt='Benchmark'></a> -->
<!-- <a href='https://huggingface.co/datasets/depth-anything/data'><img src='https://img.shields.io/badge/Benchmark-xxx-yellow' alt='Data'></a> -->

</div>

This work presents **Depth Anything 3 (DA3)**, a model that predicts spatially consistent geometry from
arbitrary visual inputs, with or without known camera poses.
In pursuit of minimal modeling, DA3 yields two key insights:
- ğŸ’ A **single plain transformer** (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization,
- âœ¨ A singular **depth-ray representation** obviates the need for complex multi-task learning.

ğŸ† DA3 significantly outperforms
[DA2](https://github.com/DepthAnything/Depth-Anything-V2) for monocular depth estimation,
and [VGGT](https://github.com/facebookresearch/vggt) for multi-view depth estimation and pose estimation.
All models are trained exclusively on **public academic datasets**.

<!-- <p align="center">
  <img src="assets/images/da3_teaser.png" alt="Depth Anything 3" width="100%">
</p> -->
<p align="center">
  <img src="assets/images/demo320-2.gif" alt="Depth Anything 3 - Left" width="70%">
</p>
<p align="center">
  <img src="assets/images/da3_radar.png" alt="Depth Anything 3" width="100%">
</p>


## ğŸ“° News
- **11-12-2025:** ğŸš€ New models and [**DA3-Streaming**](da3_streaming/README.md) released! Handle ultra-long video sequence inference with less than 12GB GPU memory via sliding-window streaming inference. Special thanks to [Kai Deng](https://github.com/DengKaiCQ) for his contribution to DA3-Streaming!
- **08-12-2025:** ğŸ“Š [Benchmark evaluation pipeline](docs/BENCHMARK.md) released! Evaluate pose estimation & 3D reconstruction on 5 datasets.
- **30-11-2025:** Add [`use_ray_pose`](#use-ray-pose) and [`ref_view_strategy`](docs/funcs/ref_view_strategy.md) (reference view selection for multi-view inputs).   
- **25-11-2025:** Add [Awesome DA3 Projects](#-awesome-da3-projects), a community-driven section featuring DA3-based applications.
- **14-11-2025:** Paper, project page, code and models are all released.

## âœ¨ Highlights

### ğŸ† Model Zoo
We release three series of models, each tailored for specific use cases in visual geometry.

- ğŸŒŸ **DA3 Main Series** (`DA3-Giant`, `DA3-Large`, `DA3-Base`, `DA3-Small`) These are our flagship foundation models, trained with a unified depth-ray representation. By varying the input configuration, a single model can perform a wide range of tasks:
  + ğŸŒŠ **Monocular Depth Estimation**: Predicts a depth map from a single RGB image.
  + ğŸŒŠ **Multi-View Depth Estimation**: Generates consistent depth maps from multiple images for high-quality fusion.
  + ğŸ¯ **Pose-Conditioned Depth Estimation**: Achieves superior depth consistency when camera poses are provided as input.
  + ğŸ“· **Camera Pose Estimation**:  Estimates camera extrinsics and intrinsics from one or more images.
  + ğŸŸ¡ **3D Gaussian Estimation**: Directly predicts 3D Gaussians, enabling high-fidelity novel view synthesis.

- ğŸ“ **DA3 Metric Series** (`DA3Metric-Large`) A specialized model fine-tuned for metric depth estimation in monocular settings, ideal for applications requiring real-world scale.

- ğŸ” **DA3 Monocular Series** (`DA3Mono-Large`). A dedicated model for high-quality relative monocular depth estimation. Unlike disparity-based models (e.g.,  [Depth Anything 2](https://github.com/DepthAnything/Depth-Anything-V2)), it directly predicts depth, resulting in superior geometric accuracy.

ğŸ”— Leveraging these available models, we developed a **nested series** (`DA3Nested-Giant-Large`). This series combines a any-view giant model with a metric model to reconstruct visual geometry at a real-world metric scale.

### ğŸ› ï¸ Codebase Features
Our repository is designed to be a powerful and user-friendly toolkit for both practical application and future research.
- ğŸ¨ **Interactive Web UI & Gallery**: Visualize model outputs and compare results with an easy-to-use Gradio-based web interface.
- âš¡ **Flexible Command-Line Interface (CLI)**: Powerful and scriptable CLI for batch processing and integration into custom workflows.
- ğŸ’¾ **Multiple Export Formats**: Save your results in various formats, including `glb`, `npz`, depth images, `ply`, 3DGS videos, etc, to seamlessly connect with other tools.
- ğŸ”§ **Extensible and Modular Design**: The codebase is structured to facilitate future research and the integration of new models or functionalities.


<!-- ### ğŸ¯ Visual Geometry Benchmark
We introduce a new benchmark to rigorously evaluate geometry prediction models on three key tasks: pose estimation, 3D reconstruction, and visual rendering (novel view synthesis) quality.

- ğŸ”„ **Broad Model Compatibility**: Our benchmark is designed to be versatile, supporting the evaluation of various models, including both monocular and multi-view depth estimation approaches.
- ğŸ”¬ **Robust Evaluation Pipeline**: We provide a standardized pipeline featuring RANSAC-based pose alignment, TSDF fusion for dense reconstruction, and a principled view selection strategy for novel view synthesis.
- ğŸ“Š **Standardized Metrics**: Performance is measured using established metrics: AUC for pose accuracy, F1-score and Chamfer Distance for reconstruction, and PSNR/SSIM/LPIPS for rendering quality.
- ğŸŒ **Diverse and Challenging Datasets**: The benchmark spans a wide range of scenes from datasets like HiRoom, ETH3D, DTU, 7Scenes, ScanNet++, DL3DV, Tanks and Temples, and MegaDepth. -->


## ğŸš€ Quick Start

### ğŸ“¦ Installation

```bash
pip install xformers torch\>=2 torchvision
pip install -e . # Basic
pip install --no-build-isolation git+https://github.com/nerfstudio-project/gsplat.git@0b4dddf04cb687367602c01196913cde6a743d70 # for gaussian head
pip install -e ".[app]" # Gradio, python>=3.10
pip install -e ".[all]" # ALL
```

For detailed model information, please refer to the [Model Cards](#-model-cards) section below.

### ğŸ’» Basic Usage

```python
import glob, os, torch
from depth_anything_3.api import DepthAnything3
device = torch.device("cuda")
model = DepthAnything3.from_pretrained("depth-anything/DA3NESTED-GIANT-LARGE")
model = model.to(device=device)
example_path = "assets/examples/SOH"
images = sorted(glob.glob(os.path.join(example_path, "*.png")))
prediction = model.inference(
    images,
)
# prediction.processed_images : [N, H, W, 3] uint8   array
print(prediction.processed_images.shape)
# prediction.depth            : [N, H, W]    float32 array
print(prediction.depth.shape)  
# prediction.conf             : [N, H, W]    float32 array
print(prediction.conf.shape)  
# prediction.extrinsics       : [N, 3, 4]    float32 array # opencv w2c or colmap format
print(prediction.extrinsics.shape)
# prediction.intrinsics       : [N, 3, 3]    float32 array
print(prediction.intrinsics.shape)
```

```bash

export MODEL_DIR=depth-anything/DA3NESTED-GIANT-LARGE
# This can be a Hugging Face repository or a local directory
# If you encounter network issues, consider using the following mirror: export HF_ENDPOINT=https://hf-mirror.com
# Alternatively, you can download the model directly from Hugging Face
export GALLERY_DIR=workspace/gallery
mkdir -p $GALLERY_DIR

# CLI auto mode with backend reuse
da3 backend --model-dir ${MODEL_DIR} --gallery-dir ${GALLERY_DIR} # Cache model to gpu
da3 auto assets/examples/SOH \
    --export-format glb \
    --export-dir ${GALLERY_DIR}/TEST_BACKEND/SOH \
    --use-backend

# CLI video processing with feature visualization
da3 video assets/examples/robot_unitree.mp4 \
    --fps 15 \
    --use-backend \
    --export-dir ${GALLERY_DIR}/TEST_BACKEND/robo \
    --export-format glb-feat_vis \
    --feat-vis-fps 15 \
    --process-res-method lower_bound_resize \
    --export-feat "11,21,31"

# CLI auto mode without backend reuse
da3 auto assets/examples/SOH \
    --export-format glb \
    --export-dir ${GALLERY_DIR}/TEST_CLI/SOH \
    --model-dir ${MODEL_DIR}

```

The model architecture is defined in [`DepthAnything3Net`](src/depth_anything_3/model/da3.py), and specified with a Yaml config file located at [`src/depth_anything_3/configs`](src/depth_anything_3/configs). The input and output processing are handled by [`DepthAnything3`](src/depth_anything_3/api.py). To customize the model architecture, simply create a new config file (*e.g.*, `path/to/new/config`) as:

```yaml
__object__:
  path: depth_anything_3.model.da3
  name: DepthAnything3Net
  args: as_params

net:
  __object__:
    path: depth_anything_3.model.dinov2.dinov2
    name: DinoV2
    args: as_params

  name: vitb
  out_layers: [5, 7, 9, 11]
  alt_start: 4
  qknorm_start: 4
  rope_start: 4
  cat_token: True

head:
  __object__:
    path: depth_anything_3.model.dualdpt
    name: DualDPT
    args: as_params

  dim_in: &head_dim_in 1536
  output_dim: 2
  features: &head_features 128
  out_channels: &head_out_channels [96, 192, 384, 768]
```

Then, the model can be created with the following code snippet.
```python
from depth_anything_3.cfg import create_object, load_config

Model = create_object(load_config("path/to/new/config"))
```



## ğŸ“š Useful Documentation

- ğŸ–¥ï¸ [Command Line Interface](docs/CLI.md)
- ğŸ“‘ [Python API](docs/API.md)
- ğŸ“Š [Benchmark Evaluation](docs/BENCHMARK.md)

## ğŸ—‚ï¸ Model Cards

Generally, you should observe that DA3-LARGE achieves comparable results to VGGT.

The Nested series uses an Any-view model to estimate pose and depth, and a monocular metric depth estimator for scaling. 

âš ï¸ Models with the `-1.1` suffix are retrained after fixing a training bug; prefer these refreshed checkpoints. The original `DA3NESTED-GIANT-LARGE`, `DA3-GIANT`, and `DA3-LARGE` remain available but are deprecated. You could expect much better performance for street scenes with the `-1.1` models.

| ğŸ—ƒï¸ Model Name                  | ğŸ“ Params | ğŸ“Š Rel. Depth | ğŸ“· Pose Est. | ğŸ§­ Pose Cond. | ğŸ¨ GS | ğŸ“ Met. Depth | â˜ï¸ Sky Seg | ğŸ“„ License     |
|-------------------------------|-----------|---------------|--------------|---------------|-------|---------------|-----------|----------------|
| **Nested** | | | | | | | | |
| [DA3NESTED-GIANT-LARGE-1.1](https://huggingface.co/depth-anything/DA3NESTED-GIANT-LARGE-1.1)  | 1.40B     | âœ…             | âœ…            | âœ…             | âœ…     | âœ…             | âœ…         | CC BY-NC 4.0   |
| [DA3NESTED-GIANT-LARGE](https://huggingface.co/depth-anything/DA3NESTED-GIANT-LARGE)  | 1.40B     | âœ…             | âœ…            | âœ…             | âœ…     | âœ…             | âœ…         | CC BY-NC 4.0   |
| **Any-view Model** | | | | | | | | |
| [DA3-GIANT-1.1](https://huggingface.co/depth-anything/DA3-GIANT-1.1)                     | 1.15B     | âœ…             | âœ…            | âœ…             | âœ…     |               |           | CC BY-NC 4.0   |
| [DA3-GIANT](https://huggingface.co/depth-anything/DA3-GIANT)                     | 1.15B     | âœ…             | âœ…            | âœ…             | âœ…     |               |           | CC BY-NC 4.0   |
| [DA3-LARGE-1.1](https://huggingface.co/depth-anything/DA3-LARGE-1.1)                     | 0.35B     | âœ…             | âœ…            | âœ…             |       |               |           | CC BY-NC 4.0     |
| [DA3-LARGE](https://huggingface.co/depth-anything/DA3-LARGE)                     | 0.35B     | âœ…             | âœ…            | âœ…             |       |               |           | CC BY-NC 4.0     |
| [DA3-BASE](https://huggingface.co/depth-anything/DA3-BASE)                     | 0.12B     | âœ…             | âœ…            | âœ…             |       |               |           | Apache 2.0     |
| [DA3-SMALL](https://huggingface.co/depth-anything/DA3-SMALL)                     | 0.08B     | âœ…             | âœ…            | âœ…             |       |               |           | Apache 2.0     |
|                               |           |               |              |               |               |       |           |                |
| **Monocular Metric Depth** | | | | | | | | |
| [DA3METRIC-LARGE](https://huggingface.co/depth-anything/DA3METRIC-LARGE)              | 0.35B     | âœ…             |              |               |       | âœ…             | âœ…         | Apache 2.0     |
|                               |           |               |              |               |               |       |           |                |
| **Monocular Depth** | | | | | | | | |
| [DA3MONO-LARGE](https://huggingface.co/depth-anything/DA3MONO-LARGE)                | 0.35B     | âœ…             |              |               |               |       | âœ…         | Apache 2.0     |


## â“ FAQ

- **Monocular Metric Depth**: To obtain metric depth in meters from `DA3METRIC-LARGE`, use `metric_depth = focal * net_output / 300.`, where `focal` is the focal length in pixels (typically the average of fx and fy from the camera intrinsic matrix K). Note that the output from `DA3NESTED-GIANT-LARGE` is already in meters.

- <a id="use-ray-pose"></a>**Ray Head (`use_ray_pose`)**:  Our API and CLI support `use_ray_pose` arg, which means that the model will derive camera pose from ray head, which is generally slightly slower, but more accurate. Note that the default is `False` for faster inference speed. 
  <details>
  <summary>AUC3 Results for DA3NESTED-GIANT-LARGE</summary>
  
  | Model | HiRoom | ETH3D | DTU | 7Scenes | ScanNet++ | 
  |-------|------|-------|-----|---------|-----------|
  | `ray_head` | 84.4 | 52.6 | 93.9 | 29.5 | 89.4 |
  | `cam_head` | 80.3 | 48.4 | 94.1 | 28.5 | 85.0 |

  </details>




- **Older GPUs without XFormers support**: See [Issue #11](https://github.com/ByteDance-Seed/Depth-Anything-3/issues/11). Thanks to [@S-Mahoney](https://github.com/S-Mahoney) for the solution!


## ğŸ¢ Awesome DA3 Projects

A community-curated list of Depth Anything 3 integrations across 3D tools, creative pipelines, robotics, and web/VR viewers, including but not limited to these. You are welcome to submit your DA3-based project via PR, and we will review and feature it if applicable.

- [DA3-blender](https://github.com/xy-gao/DA3-blender): Blender addon for DA3-based 3D reconstruction from a set of images. 

- [ComfyUI-DepthAnythingV3](https://github.com/PozzettiAndrea/ComfyUI-DepthAnythingV3): ComfyUI nodes for Depth Anything 3, supporting single/multi-view and video-consistent depth with optional pointâ€‘cloud export.

- [DA3-ROS2-Wrapper](https://github.com/GerdsenAI/GerdsenAI-Depth-Anything-3-ROS2-Wrapper): Real-time DA3 depth in ROS2 with multi-camera support. 

- [DA3-ROS2-CPP-TensorRT](https://github.com/ika-rwth-aachen/ros2-depth-anything-v3-trt): DA3 ROS2 C++ TensorRT Inference Node: a ROS2 node for DA3 depth estimation using TensorRT for real-time inference.

- [VideoDepthViewer3D](https://github.com/amariichi/VideoDepthViewer3D): Streaming videos with DA3 metric depth to a Three.js/WebXR 3D viewer for VR/stereo playback.


## ğŸ§‘â€ğŸ’» Official Codebase Core Contributors and Maintainers

<table>
  <tr>
    <td align="center">
      <a href="https://bingykang.github.io/">
        <img src="https://images.weserv.nl/?url=https://bingykang.github.io/images/bykang_homepage.jpeg?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub><b>Bingyi Kang</b></sub>
    </td>
    <td align="center">
      <a href="https://haotongl.github.io/">
        <img src="https://images.weserv.nl/?url=https://haotongl.github.io/assets/img/prof_pic.jpg?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Haotong Lin</sub>
    </td>
    <td align="center">
      <a href="https://github.com/SiliChen321">
        <img src="https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/195901058?v=4&h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Sili Chen</sub>
    </td>
    <td align="center">
      <a href="https://liewjunhao.github.io/">
        <img src="https://images.weserv.nl/?url=https://liewjunhao.github.io/images/liewjunhao.png?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
       </a>
        <br />
        <sub>Jun Hao Liew</sub>
    </td>
    <td align="center">
      <a href="https://donydchen.github.io/">
        <img src="https://images.weserv.nl/?url=https://donydchen.github.io/assets/img/profile.jpg?h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Donny Y. Chen</sub>
    </td>
    <td align="center">
      <a href="https://github.com/DengKaiCQ">
        <img src="https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/59907452?v=4&h=100&w=100&fit=cover&mask=circle&maxage=7d" width="100px;" alt=""/>
      </a>
        <br />
        <sub>Kai Deng</sub>
    </td>
  </tr>
</table>

## ğŸŒ¾ ç¨²ç¾¤è½ãƒãƒƒãƒæ¨è«–ï¼†åˆ†é›¢ãƒ„ãƒ¼ãƒ«ï¼ˆGUIï¼‰

Depth-Anything-3 ã‚’ç”¨ã„ã¦ã€é‰›ç›´çœŸä¸Šã‹ã‚‰æ’®å½±ã—ãŸæ°´ç¨²ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã€ä¸€æ‹¬ã§
- æ·±åº¦æ¨å®šï¼ˆå¯è¦–åŒ–PNGä¿å­˜ï¼‰
- æ·±åº¦ã«åŸºã¥ãã‚¤ãƒ/åœ°é¢åˆ†é›¢ï¼ˆ2å€¤ãƒã‚¹ã‚¯PNGä¿å­˜ï¼‰
- ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤å¯è¦–åŒ–PNGä¿å­˜
- æ¤è¢«ç‡ï¼ˆå„ç”»åƒãƒ»å…¨ä½“ï¼‰ã®CSVä¿å­˜

ã‚’è¡Œã† Gradio ãƒ™ãƒ¼ã‚¹ã®GUIãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚

- èµ·å‹•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: `depth_anything_3.app.rice_canopy_app`
- å…¥å‡ºåŠ›ã®ä¿å­˜è¦ç´„:
  - å…¥åŠ›ä¾‹: `base_dir/images/shooting_date`
  - å‡ºåŠ›:
    - æ·±åº¦å¯è¦–åŒ–: `base_dir/depth_images/shooting_date`
    - 2å€¤ãƒã‚¹ã‚¯: `base_dir/seg_images/shooting_date`
    - ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤: `base_dir/overlay_images/shooting_date`
    - CSV: `base_dir/reports/shooting_date/coverage.csv`
  - å…¥åŠ›ãŒä¸Šè¨˜è¦ç´„ã§ãªãã¦ã‚‚ã€å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®è¦ªé…ä¸‹ã«å…„å¼Ÿãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã—ã¦åŒæ§˜ã®æ§‹é€ ã‚’ä½œæˆã—ã¾ã™

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆWSL/ä»®æƒ³ç’°å¢ƒæ¨å¥¨ï¼‰

1) ä»®æƒ³ç’°å¢ƒ
```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
```

2) ä¾å­˜é–¢ä¿‚ï¼ˆtorchç³»ã‚’é™¤ãï¼‰
```bash
pip install -r requirements.txt
```
- æœ¬ãƒªãƒã‚¸ãƒˆãƒªã® `requirements.txt` ã§ã¯ã€WSL/ç’°å¢ƒå·®ã«å¯¾å¿œã™ã‚‹ãŸã‚ `torch/torchvision` ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã„ã¾ã™

3) PyTorch ã®æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦é¸æŠï¼‰
- CUDA 12.1:
```bash
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio
```
- CUDA 12.4:
```bash
pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio
```
- CPU ã®ã¿:
```bash
pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio
```
- å…¬å¼æ¡ˆå†…: https://pytorch.org/get-started/locally/

è£œè¶³:
- `xformers` ã¯CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•´åˆãŒå¿…è¦ã§ã™ã€‚ãƒ“ãƒ«ãƒ‰ã«å¤±æ•—ã™ã‚‹å ´åˆã¯ä¸€æ—¦ `requirements.txt` ã‹ã‚‰å¤–ã™/ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã‚‚æ¨è«–ã¯å¤šãã®ã‚±ãƒ¼ã‚¹ã§å‹•ä½œã—ã¾ã™
- `pip install -e .` ã‚’ä½¿ã†å ´åˆã¯ `pyproject.toml` ã« `torch` ä¾å­˜ãŒã‚ã‚‹ãŸã‚ `--no-deps` ã®åˆ©ç”¨ã‚’æ¨å¥¨ã—ã¾ã™:
```bash
pip install -e . --no-deps
```

### å®Ÿè¡Œ

```bash
python -m depth_anything_3.app.rice_canopy_app
```

- ãƒ–ãƒ©ã‚¦ã‚¶ã§èµ·å‹•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: http://127.0.0.1:7861ï¼‰
- ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ï¼ˆjpg/pngã®ã¿ï¼‰ã‚’å…¥åŠ›
- ãƒ¢ãƒ‡ãƒ«: `da3-large`ï¼ˆç›¸å¯¾æ·±åº¦; æ—¢å®šï¼‰
  - å°†æ¥æ‹¡å¼µã¨ã—ã¦ `da3metric-large` ã‚’é¸æŠè‚¢ã«ç”¨æ„ï¼ˆUIã‹ã‚‰é¸æŠå¯èƒ½ï¼‰
- åˆ†é›¢: æ—¢å®šã¯ Otsuï¼ˆæµ…ã„=ã‚¤ãƒï¼‰ã€‚å¿…è¦ãªã‚‰ã€Œåè»¢ï¼ˆæ·±ã„=ã‚¤ãƒï¼‰ã€ã‚’ON
- ä¿å­˜ç‰©:
  - æ·±åº¦å¯è¦–åŒ–PNGï¼ˆ8bit, viridis/turboï¼‰
  - 2å€¤ãƒã‚¹ã‚¯PNGï¼ˆ0/255ï¼‰
  - ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤PNGï¼ˆç·‘ã§åŠé€æ˜åˆæˆï¼‰
  - CSVï¼ˆå„ç”»åƒã®æ¤è¢«ç‡ã¨ overallï¼‰

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `process_res` / `process_res_method`: æ¨è«–è§£åƒåº¦ãƒ»æ–¹æ³•
- `batch_size`: VRAMã«å¿œã˜ã¦èª¿æ•´
- `min_area` / `close_kernel`: å°é ˜åŸŸé™¤å»/ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°
- `alpha_overlay`: ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤é€éç‡

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£èª¬ï¼ˆæ¨å¥¨å€¤ï¼‰
- ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
  - Windowsã®ãƒ‘ã‚¹ã§ã‚‚å¯ï¼ˆä¾‹: C:\path\to\dir ã‚„ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ï¼‰ã€‚WSLã§ã¯è‡ªå‹•çš„ã« /mnt/... ã¸æ­£è¦åŒ–ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãŸå ´åˆã¯è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’å‡¦ç†å¯¾è±¡ã«åˆ‡æ›¿ã€‚
- ãƒ¢ãƒ‡ãƒ«ï¼ˆda3-large æ¨å¥¨ï¼‰
  - ç›¸å¯¾æ·±åº¦ãƒ¢ãƒ‡ãƒ«ã€‚å°†æ¥ da3metric-largeï¼ˆå®Ÿã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã«ã‚‚å¯¾å¿œå¯èƒ½ãªãƒ•ãƒƒã‚¯å®Ÿè£…æ¸ˆã¿ã€‚
- Model Repo ä¸Šæ›¸ã
  - æ—¢å®š: depth-anything/DA3-LARGEã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯äº‹æƒ…ã«å¿œã˜ã¦ãƒŸãƒ©ãƒ¼ã‚„ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´å¯ã€‚
- ãƒ‡ãƒã‚¤ã‚¹
  - autoï¼ˆæ—¢å®šï¼‰/cuda/cpuã€‚autoã¯CUDAå¯ç”¨æ™‚ã¯GPUã‚’é¸æŠã€‚
- process_res / process_res_method
  - æ¨è«–è§£åƒåº¦ã¨ãƒªã‚µã‚¤ã‚ºæ–¹æ³•ã€‚æ—¢å®š: 504 / upper_bound_resizeã€‚é«˜ç²¾ç´°ã‚’å„ªå…ˆã™ã‚‹å ´åˆã¯ 720ã€œ1024 ã‚„ high_res ã‚’æ¤œè¨ã€‚VRAMã«å¿œã˜ã¦èª¿æ•´ã€‚
- ãƒãƒƒãƒã‚µã‚¤ã‚º
  - æ—¢å®š: 8ã€‚GPUã§ã¯2ã€œ8ç¨‹åº¦ã€CPUã§ã¯1ã€œ2ç¨‹åº¦ãŒç›®å®‰ã€‚OOMã‚„é€Ÿåº¦ã«å¿œã˜ã¦èª¿æ•´ã€‚
- åˆ†é›¢æ‰‹æ³•ï¼ˆotsu/manualï¼‰
  - æ—¢å®š: otsuï¼ˆå¤§åŸŸã—ãã„ã€è‡ªå‹•ï¼‰ã€‚çµæœãŒå°‘ãªã™ã/å¤šã™ãã‚‹å ´åˆã¯ã€Œåè»¢ã€ã‚’åˆ‡æ›¿ã€ã¾ãŸã¯ manual ã§ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ï¼ˆ0..1ï¼‰ã‚’0.3/0.5/0.7ãªã©è©¦è¡Œã€‚
- åè»¢ï¼ˆæ·±ã„=ã‚¤ãƒï¼‰
  - æ—¢å®š: OFFï¼ˆæµ…ã„=ã‚¤ãƒï¼‰ã€‚æ’®å½±æ¡ä»¶ã§é€†ã«ãªã‚‹å ´åˆã¯ONã€‚
- æœ€å°é ˜åŸŸãƒ”ã‚¯ã‚»ãƒ«ï¼ˆmin_areaï¼‰
  - æ—¢å®š: 200ã€‚å°ã•ã„ãƒã‚¤ã‚ºé ˜åŸŸã‚’å‰Šé™¤ã€‚ãƒã‚¤ã‚ºãŒå¤šã‘ã‚Œã°ä¸Šã’ã‚‹ï¼ˆä¾‹: 500ã€œ2000ï¼‰ã€‚
- ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã‚«ãƒ¼ãƒãƒ«ï¼ˆclose_kernelï¼‰
  - æ—¢å®š: 3ã€‚ç©´åŸ‹ã‚ãƒ»ã‚®ãƒ£ãƒƒãƒ—é–‰ã˜ã€‚æ»‘ã‚‰ã‹ã«ã—ãŸã‘ã‚Œã°5ã€œ9ã‚‚æ¤œè¨ã€‚0ã§ç„¡åŠ¹ã€‚
- æ·±åº¦ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ï¼ˆcmapï¼‰
  - å¯è¦–åŒ–ã®ã¿ï¼ˆviridis/turboï¼‰ã€‚çµæœã«ã¯å½±éŸ¿ãªã—ã€‚
- ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤é€éç‡ï¼ˆalpha_overlayï¼‰
  - æ—¢å®š: 0.5ã€‚é‡ã­è¡¨ç¤ºã®è¦‹ã‚„ã™ã•èª¿æ•´ã€‚
- overallè¡Œä»˜ãCSVã‚‚ä¿å­˜ã™ã‚‹
  - ONæ™‚ã€per-imageã«åŠ ãˆã¦OVERALLè¡Œä»˜ãã®coverage.csvã‚‚ä¿å­˜ã€‚
- ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆä¾‹å¤–è©³ç´°ï¼‰
  - ONæ™‚ã€ä¾‹å¤–ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’ãƒ­ã‚°ã«è¿½è¨˜ã€‚ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«log.txtã¨ã—ã¦ã‚‚ä¿å­˜ã€‚

### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€
- æ·±åº¦å¯è¦–åŒ–PNG: base_dir/depth_images/shooting_date/xxx_depth.png
- 2å€¤ãƒã‚¹ã‚¯PNG: base_dir/seg_images/shooting_date/xxx_plant.png
- ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤PNG: base_dir/overlay_images/shooting_date/xxx_overlay.png
- CSVï¼ˆç”»åƒã”ã¨ã®æ¤è¢«ç‡ï¼‰: base_dir/reports/shooting_date/coverage_per_image.csv
  - åˆ—: filename,width,height,plant_px,valid_px,coverage_percent
- CSVï¼ˆOVERALLè¡Œä»˜ãã€ä»»æ„ï¼‰: base_dir/reports/shooting_date/coverage.csv
- ãƒ‡ãƒãƒƒã‚°/å‡¦ç†ãƒ­ã‚°: base_dir/reports/shooting_date/log.txt

### æ³¨æ„ç‚¹

- Hugging Face ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ¶ç´„ãŒã‚ã‚‹å ´åˆã¯ãƒŸãƒ©ãƒ¼ï¼ˆ`HF_ENDPOINT`ï¼‰ã®åˆ©ç”¨ã‚’ã”æ¤œè¨ãã ã•ã„
- æ—¢å®šã®ãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒ: `depth-anything/DA3-LARGE`ï¼ˆUIã§ä¸Šæ›¸ãå¯ï¼‰
- ç”»åƒã¯ `.jpg/.jpeg/.png` ã®ã¿å‡¦ç†ã—ã¾ã™

---

### ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒˆ

- ã‚¨ãƒ©ãƒ¼: `ModuleNotFoundError: No module named 'depth_anything_3'`
  - åŸå› : æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Œsrcãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€ã§ã™ã€‚`python -m depth_anything_3....` ã‚’ä½¿ã†å ´åˆã€`src` ã‚’Pythonã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ¢ç´¢çµŒè·¯ã«è¼‰ã›ã‚‹ã‹ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
  - å¯¾å‡¦1ï¼ˆæ¨å¥¨, ä¾å­˜è§£æ±ºã¯æ‰‹å‹•ã®ãŸã‚ --no-depsï¼‰:
    ```bash
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã§
    pip install -e . --no-deps
    # ãã®å¾Œ
    python -m depth_anything_3.app.rice_canopy_app
    ```
    - WSL/ä»®æƒ³ç’°å¢ƒã§torchã¯æ‰‹å‹•å°å…¥æ–¹é‡ã®ãŸã‚ã€`--no-deps` ã‚’ä»˜ã‘ã¦ãã ã•ã„ï¼ˆtorch/torchvisionãŒè‡ªå‹•ã§å…¥ã‚‰ãªã„ã‚ˆã†ã«ï¼‰ã€‚
  - å¯¾å‡¦2ï¼ˆç’°å¢ƒå¤‰æ•°ã§PYTHONPATHã‚’ä¸€æ™‚è¿½åŠ ï¼‰:
    ```bash
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã§
    export PYTHONPATH="$PWD/src:$PYTHONPATH"
    python -m depth_anything_3.app.rice_canopy_app
    ```
  - å¯¾å‡¦3ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯ãªãã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç›´æ¥å®Ÿè¡Œï¼‰:
    ```bash
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã§
    python src/depth_anything_3/app/rice_canopy_app.py
    ```

- ãªãŠã€WSLä¸Šã§ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–ã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„:
  ```bash
  source .venv/bin/activate
  ```

## ğŸ“ Citations
If you find Depth Anything 3 useful in your research or projects, please cite our work:

```
@article{depthanything3,
  title={Depth Anything 3: Recovering the visual space from any views},
  author={Haotong Lin and Sili Chen and Jun Hao Liew and Donny Y. Chen and Zhenyu Li and Guang Shi and Jiashi Feng and Bingyi Kang},
  journal={arXiv preprint arXiv:2511.10647},
  year={2025}
}
```
